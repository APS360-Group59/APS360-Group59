{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Primary and Baseline Model",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdIaLZ7YwQGF"
      },
      "source": [
        "#Project - Primary Model & Baseline Model <img src=\"https://upload.wikimedia.org/wikipedia/en/thumb/0/04/Utoronto_coa.svg/1024px-Utoronto_coa.svg.png\" width=60px align=\"right\"> \n",
        "###APS360 - Applied Fundamentals of Machine Learning\n",
        "\n",
        "---\n",
        "**Members**\n",
        "- Javiera Bao\n",
        "- Kieran Kasha\n",
        "- Rishik Kumar\n",
        "- Abhay Verma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dN-Bj96L7ti"
      },
      "source": [
        "## 0. Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m86404HdL8WL"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset \n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision.transforms as transforms\n",
        "import pandas as pd\n",
        "from skimage import io\n",
        "from math import floor\n",
        "from PIL import Image\n",
        "import seaborn as sn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "use_cuda = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzJo6QbC_vkA",
        "outputId": "534ea1ed-53da-4521-9fd9-9c74623e6697"
      },
      "source": [
        "# Mounting Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIDmNKy1JaEE"
      },
      "source": [
        "## 1. Getting Preprocessed Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUuBN7oTMGcM"
      },
      "source": [
        "# Image Dataset Creator\n",
        "\n",
        "class SoundFilesDataset (Dataset):\n",
        "    def __init__(self, csv_file, transform = None):\n",
        "        self.annotations = pd.read_csv(csv_file)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_path = '/content/gdrive/Shareddrives/APS360/spectrograms/' + str(self.annotations.iloc[index, 1]) + '.png'\n",
        "        image = io.imread(image_path)\n",
        "        label = (self.annotations.iloc[index, 3])\n",
        "        \n",
        "        if label == 'acoustic guitar':\n",
        "            label = 0 #acousitc guitar\n",
        "        elif label == 'cello':\n",
        "            label = 1 #cello\n",
        "        elif label == 'clarinet':\n",
        "            label = 2 #clarinet\n",
        "        elif label == 'double bass':\n",
        "            label = 3 #double bass\n",
        "        elif label == 'drum set':\n",
        "            label = 4 #drum set\n",
        "        elif label == 'flute':\n",
        "            label = 5 #flute\n",
        "        elif label == 'piano':\n",
        "            label = 6 #piano\n",
        "        elif label == 'viola':\n",
        "            label = 7 #viola\n",
        "        elif label == 'violin':\n",
        "            label = 8 #violin\n",
        "        elif label == 'singer':\n",
        "            label = 9 #singer\n",
        "        \n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        return (image, label)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0K_5L9NjJVut"
      },
      "source": [
        "dataset_training = SoundFilesDataset(\"/content/gdrive/Shareddrives/APS360/final_csv/train.csv\", transform= transforms.ToTensor())\n",
        "dataset_validation = SoundFilesDataset(\"/content/gdrive/Shareddrives/APS360/final_csv/valid.csv\", transform= transforms.ToTensor())\n",
        "dataset_testing = SoundFilesDataset(\"/content/gdrive/Shareddrives/APS360/final_csv/test.csv\", transform= transforms.ToTensor())\n",
        "\n",
        "num_workers = 0\n",
        "batch_size = 5\n",
        "\n",
        "training_loader = torch.utils.data.DataLoader(dataset_training, batch_size=batch_size, num_workers=num_workers, shuffle = True)\n",
        "validation_loader = torch.utils.data.DataLoader(dataset_validation, batch_size=batch_size, num_workers=num_workers, shuffle = True)\n",
        "testing_loader = torch.utils.data.DataLoader(dataset_testing, batch_size=batch_size, num_workers=num_workers, shuffle = True)\n",
        "fitting_loader = torch.utils.data.DataLoader(dataset_fitting, batch_size=batch_size, num_workers=num_workers, shuffle = True)\n",
        "Kieran_loader = torch.utils.data.DataLoader(dataset_Kieran, batch_size=batch_size, num_workers=num_workers, shuffle = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_12oxGTu7r1A"
      },
      "source": [
        "## 2. Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2fJxQEp7vPF"
      },
      "source": [
        "torch.manual_seed(50)\n",
        "class InstrumentClassifierBaseline(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(InstrumentClassifierBaseline, self).__init__()\n",
        "        \n",
        "        self.name = 'InstrumentClassifierBaseline'\n",
        "\n",
        "        self.fc1 = nn.Linear(4*432*288, 1000) #input, output\n",
        "        self.fc2 = nn.Linear(1000, 750) #input, output\n",
        "        self.fc3 = nn.Linear(750, 500) #input, output\n",
        "        self.fc4 = nn.Linear(500, 200) #input, output\n",
        "        self.fc5 = nn.Linear(200, 100) #input, output\n",
        "        self.fc6 = nn.Linear(100, 10) #input, output\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 288*432*100)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = F.relu(self.fc4(x))\n",
        "        x = F.relu(self.fc5(x))\n",
        "        x = self.fc6(x) #Softmax is applied in training and get_accuracy\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pmtpy-m8xkLe"
      },
      "source": [
        "## 3. Primary Model: CNN Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sj873QXEKsR"
      },
      "source": [
        "torch.manual_seed(50)\n",
        "class InstrumentClassifierCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        self.kernelsizes = [5, 5, 5, 3, 3, 3]\n",
        "        super(InstrumentClassifierCNN, self).__init__()\n",
        "        \n",
        "        self.name = 'InstrumentClassifierCNN'\n",
        "\n",
        "        self.conv1 = nn.Conv2d(4, 25, self.kernelsizes[0]) #in_channels, out_chanels, kernel_size\n",
        "        self.conv2 = nn.Conv2d(25, 40, self.kernelsizes[1]) #in_channels, out_chanels, kernel_size\n",
        "        self.conv3 = nn.Conv2d(40, 60, self.kernelsizes[2]) #in_channels, out_chanels, kernel_size\n",
        "        self.conv4 = nn.Conv2d(60, 75, self.kernelsizes[3]) #in_channels, out_chanels, kernel_size\n",
        "        self.conv5 = nn.Conv2d(75, 85, self.kernelsizes[4]) #in_channels, out_chanels, kernel_size\n",
        "        self.conv6 = nn.Conv2d(85, 100, self.kernelsizes[5]) #in_channels, out_chanels, kernel_size\n",
        "        self.pool = nn.MaxPool2d(2, 2) #kernel_size, stride \n",
        "\n",
        "        # calculating the width and height of the final output from conv6\n",
        "        self.width1 = (432 - self.kernelsizes[0] + 1)/2\n",
        "        self.height1 = (288 - self.kernelsizes[0] + 1)/2\n",
        "        self.width2 = ((self.width1 - self.kernelsizes[1] + 1)/2)\n",
        "        self.height2 = ((self.height1 - self.kernelsizes[1] + 1)/2)    \n",
        "        self.width3 = (self.width2 - self.kernelsizes[2] + 1)/2\n",
        "        self.height3 = (self.height2 - self.kernelsizes[2] + 1)/2\n",
        "        self.width4 = (self.width3 - self.kernelsizes[3] + 1)/2\n",
        "        self.height4 = (self.height3 - self.kernelsizes[3] + 1)/2  \n",
        "        self.width5 = (self.width4 - self.kernelsizes[4] + 1)/2\n",
        "        self.height5 = (self.height4 - self.kernelsizes[4] + 1)/2  \n",
        "        self.width6 = int((self.width5 - self.kernelsizes[5] + 1)/2)\n",
        "        self.height6 = int((self.height5 - self.kernelsizes[5] + 1)/2)    \n",
        " \n",
        "        self.fc1 = nn.Linear(self.height6*self.width6*100, 150) #input, output\n",
        "        self.fc2 = nn.Linear(150, 10) #input, output\n",
        "       \n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = self.pool(F.relu(self.conv4(x)))\n",
        "        x = self.pool(F.relu(self.conv5(x)))\n",
        "        x = self.pool(F.relu(self.conv6(x)))\n",
        "        x = x.view(-1, self.height6*self.width6*100)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x) #softmax is applied in training and get_accuracy\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zTdoPsrxrB3"
      },
      "source": [
        "## 4. Function for Finding Accuracy, F1-score, and Generating a Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LWz0w7OUFG8"
      },
      "source": [
        "def get_accuracy(model, loader, print_data = False, conf_mtx = False, f1 = False):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    counter = 10\n",
        "\n",
        "    ## Music Class Dictionary\n",
        "\n",
        "    classes = {}\n",
        "    classes[0] = 'acoustic guitar'\n",
        "    classes[1] = 'cello'\n",
        "    classes[2] = 'clarinet'\n",
        "    classes[3] = 'double bass'\n",
        "    classes[4] = 'drum set'\n",
        "    classes[5] = 'flute'\n",
        "    classes[6] = 'piano'\n",
        "    classes[7] = 'viola'\n",
        "    classes[8] = 'violin'\n",
        "    classes[9] = 'singer'\n",
        "\n",
        "    instruments = ['Acoustic Guitar', 'Cello', 'Clarinet', 'Double Bass', 'Drum Set', 'Flute', 'Piano', 'Viola', 'Violin', 'Singer']\n",
        "\n",
        "    # For the Confusion Matrix:\n",
        "    true_val = []\n",
        "    pred_val = []\n",
        "\n",
        "\n",
        "    for img, labels in loader:\n",
        "\n",
        "        if use_cuda and torch.cuda.is_available():\n",
        "          img = img.cuda()\n",
        "          labels = labels.cuda()\n",
        "\n",
        "        # getting the prediction and true label\n",
        "        output = F.softmax(model(img), dim=1)\n",
        "        prediction = output.max(1, keepdim=True)[1]\n",
        "        correct += prediction.eq(labels.view_as(prediction)).sum().item()\n",
        "        total += img.shape[0]\n",
        "\n",
        "        #converting the prediction and true label to corresponding strings\n",
        "        pred = classes[prediction[0][0].item()]\n",
        "        corr = classes[labels[0].item()]\n",
        "\n",
        "        # to print first eleven predictions and true labels if needed\n",
        "        if print_data:\n",
        "            if counter > 0:\n",
        "                print(\"True instrument: \" + corr)\n",
        "                print(\"Predicted instrument: \" + pred + '\\n')\n",
        "            else: \n",
        "                counter -= 1\n",
        "\n",
        "        # to calculate the F1 score of the distribution\n",
        "        if f1:\n",
        "          for i in range(len(labels)):\n",
        "                true_val.append(labels[i].item())\n",
        "                pred_val.append(prediction[i][0].item())\n",
        "                if loader == training_loader:\n",
        "                  f1 = f1_score(true_val, pred_val, average='weighted')\n",
        "                elif loader == validation_loader:\n",
        "                  f1 = f1_score(true_val, pred_val, average='weighted') \n",
        "                elif loader == testing_loader:\n",
        "                  f1 = f1_score(true_val, pred_val, average='weighted')\n",
        "                \n",
        "          print(\"f1 score:\", f1)\n",
        "\n",
        "        # to display the confusion matrix for the predictions\n",
        "        if conf_mtx:\n",
        "            for i in range(len(labels)):\n",
        "                true_val.append(labels[i].item())\n",
        "                pred_val.append(prediction[i][0].item())\n",
        "        \n",
        "    if conf_mtx:\n",
        "        # Plot non-normalized confusion matrix\n",
        "        matrix = confusion_matrix(true_val, pred_val, normalize= 'true')\n",
        "        matrix_pandas = pd.DataFrame(matrix, index = instruments, columns = instruments)\n",
        "        plt.figure(figsize = (10,7))\n",
        "        ax = plt.axes()\n",
        "        ax.set_title('Confusion Matrix')\n",
        "        heatmap = sn.heatmap(matrix_pandas, ax = ax, annot=True)\n",
        "        plt.xlabel(\"Predicted Label\") \n",
        "        plt.ylabel(\"True Label\") \n",
        "        plt.show()\n",
        "\n",
        "    result = correct/total\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdWuni6m8XHO"
      },
      "source": [
        "## 5. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "II-YhZn03owR"
      },
      "source": [
        "def train(model, trainingdataloader, validationdataloader, batch_size=64, num_epochs=30, lr=0.01):\n",
        "\n",
        "    torch.manual_seed(50)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    training_accuracy = np.zeros(num_epochs)\n",
        "    validation_accuracy = np.zeros(num_epochs)\n",
        "\n",
        "    losses = []\n",
        "    \n",
        "    # Now the training loops can begin.\n",
        "    for epochs in range(num_epochs):\n",
        "        total_training_err = 0.0\n",
        "        total_training_loss = 0.0\n",
        "        total_imgs = 0\n",
        "        for imgs, labels in iter(trainingdataloader):\n",
        "            \n",
        "            # This code allows me to use the GPU\n",
        "            if use_cuda and torch.cuda.is_available():\n",
        "              imgs = imgs.cuda()\n",
        "              labels = labels.cuda()\n",
        "\n",
        "            # This code is essentially identical to tutorial 3a\n",
        "            # This is the forward pass\n",
        "            output = model(imgs)\n",
        "            total_loss = criterion(output, labels)\n",
        "            # This is the backward pass\n",
        "            total_loss.backward()\n",
        "            # Parameter updater\n",
        "            optimizer.step()   \n",
        "            # Clean-up step for pytorch\n",
        "            optimizer.zero_grad()     \n",
        "        \n",
        "        losses.append(float(total_loss)/batch_size)  \n",
        "        # Now all that's left is to calculate the accuracy.\n",
        "        training_accuracy[epochs] = get_accuracy(model, trainingdataloader)\n",
        "        validation_accuracy[epochs] = get_accuracy(model, validationdataloader)\n",
        "        \n",
        "        # A print statement allows for better visualization of the results.\n",
        "        print((\"Epoch Number {}: Training Accuracy: {}\" + \" Validation Accuracy: {}\").format(epochs + 1, training_accuracy[epochs], validation_accuracy[epochs]))\n",
        "\n",
        "    epoch = np.arange(1, num_epochs+1)\n",
        "\n",
        "    # Plotting Training Loss Curve\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.plot(epoch, losses, label=\"Train\")\n",
        "    plt.xlabel(\"Epoch Number\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "    # Plotting Training vs Validation Curve\n",
        "    plt.title(\"Training vs Validation Curve\")\n",
        "    plt.plot(epoch, training_accuracy, label = 'Train')\n",
        "    plt.plot(epoch, validation_accuracy, label = 'Validation')\n",
        "    plt.xlabel(\"Epoch Number\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.show()\n",
        "\n",
        "    model_path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(model.name, batch_size, lr, epochs)\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    print(\"Final Training Accuracy: {}\".format(training_accuracy[-1]))\n",
        "    print(\"Final Validation Accuracy: {}\".format(validation_accuracy[-1]))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCWUvvdK8d_W"
      },
      "source": [
        "# 6. Training and Testing Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9k0dUAl7AvLc"
      },
      "source": [
        "# Training Primary Model\n",
        "model1 = InstrumentClassifierCNN()\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "    model1.cuda()\n",
        "\n",
        "train(model1, training_loader, validation_loader, batch_size = 30, lr = 0.65e-4, num_epochs = 40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M29y8O6WcZK4"
      },
      "source": [
        "# Testing Primary Model\n",
        "print('Testing Accuracy: ', get_accuracy(model1, testing_loader, print_data= True, conf_mtx= True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIJbAMdOjCi_"
      },
      "source": [
        "# Training Baseline Model\n",
        "baseline = InstrumentClassifierCNN()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    baseline.cuda()\n",
        "\n",
        "train(baseline, training_loader, validation_loader, batch_size = 30, lr = 0.8e-4, num_epochs = 24)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4970prmkjt7"
      },
      "source": [
        "# Testing Baseline Model\n",
        "print(get_accuracy(baseline, testing_loader, conf_mtx= True, f1 = True))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}